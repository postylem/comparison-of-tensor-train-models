{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensornetworks_pytorch.TNModels import PosMPS, Born\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0) / 1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0) / 1024**3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import data\n",
    "import pickle\n",
    "for dataset in ['biofam','flare','lymphography','spect','tumor','votes']:\n",
    "    with open('datasets/'+dataset, 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1' # biofam seemed to have an encoding issue which this fixes\n",
    "        a = u.load()\n",
    "    X=a[0].astype(int)\n",
    "    print(f\"╭──{dataset}\")\n",
    "    print(f\"│ data shape:{X.shape}\")\n",
    "    print(f\"│ range of X values: {X.min()} -- {X.max()}\")\n",
    "    print(f\"╰───────────────────\")\n",
    "    \n",
    "def load_dataset(dataset):\n",
    "    with open('datasets/'+dataset, 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        a = u.load()\n",
    "    X=a[0]\n",
    "    X=X.astype(int)\n",
    "    print(f\"╭──{dataset}\")\n",
    "    print(f\"│ data shape:{X.shape}\")\n",
    "    print(f\"│ range of X values: {X.min()} -- {X.max()}\")\n",
    "    print(f\"╰───────────────────\")\n",
    "    d = X.max()+1\n",
    "    return X, d\n",
    "\n",
    "def train_models(models, batchsize=20, max_epochs=50, optimizer=torch.optim.Adadelta, **optim_kwargs):\n",
    "    print(f\"dataset: {dataset}\")\n",
    "    models_loss_values={}\n",
    "    for model in models:\n",
    "        loss_values = model.train(\n",
    "            batchsize=batchsize, max_epochs=max_epochs, tqdm=tqdm, plot=False,\n",
    "            optimizer=optimizer, **optim_kwargs)\n",
    "        plt.plot(loss_values, label=model.name)\n",
    "        plt.ylabel('avg loss (NLL)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.title(f\"dataset: {dataset} (d={d}), bond dim={D}\\n batchsize:{batchsize}, {optimizer.__module__} {optim_kwargs}\")\n",
    "        models_loss_values[f\"{model.name}\"]=loss_values\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.show()\n",
    "    return models_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize models\n",
    "dataset = 'tumor'\n",
    "X,d = load_dataset(dataset)\n",
    "D = 8\n",
    "\n",
    "wr = 'noisy' # choose either None, 'noisy', or 'random_angle'\n",
    "\n",
    "mps      = PosMPS(X, d, D, homogeneous=False, w_randomization=wr)\n",
    "mps_hom  = PosMPS(X, d, D, homogeneous=True, w_randomization=wr)\n",
    "# mps_s    = PosMPS(X, d, D, homogeneous=False, log_stability=True, w_randomization=wr)\n",
    "# mps_s_hom= PosMPS(X, d, D, homogeneous=True, log_stability=True, w_randomization=wr)\n",
    "rBorn      = Born(X, d, D, dtype=torch.float, homogeneous=False, log_stability=False, w_randomization=wr) \n",
    "rBorn_hom  = Born(X, d, D, dtype=torch.float, homogeneous=True, log_stability=False, w_randomization=wr)\n",
    "rBorn_s    = Born(X, d, D, dtype=torch.float, homogeneous=False, log_stability=True, w_randomization=wr)\n",
    "rBorn_s_hom= Born(X, d, D, dtype=torch.float, homogeneous=True, log_stability=True, w_randomization=wr)\n",
    "cBorn      = Born(X, d, D, dtype=torch.cfloat, homogeneous=False, log_stability=False, w_randomization=wr)\n",
    "cBorn_hom  = Born(X, d, D, dtype=torch.cfloat, homogeneous=True, log_stability=False, w_randomization=wr)\n",
    "cBorn_s    = Born(X, d, D, dtype=torch.cfloat, homogeneous=False, log_stability=True, w_randomization=wr)\n",
    "cBorn_s_hom= Born(X, d, D, dtype=torch.cfloat, homogeneous=True, log_stability=True, w_randomization=wr)\n",
    "models     = (rBorn, cBorn, rBorn_s, cBorn_s, mps)#, mps_c\n",
    "models_hom = (rBorn_hom, cBorn_hom, rBorn_s_hom, cBorn_s_hom, mps_hom)#, mps_c_hom\n",
    "\n",
    "def clip_grad(grad, clip_val, param_name, verbose=False):\n",
    "    \"\"\"Clip the gradients, to be used as a hook during training.\"\"\"\n",
    "    if torch.isnan(grad).any():\n",
    "        print(f\"├─NaN value in gradient of {param_name}, {grad.size()}\")\n",
    "    if grad.dtype==torch.cfloat:\n",
    "        for ext, v in [(\"min\", grad.real.min()),(\"max\", grad.real.max())]:\n",
    "            if verbose and abs(v) > clip_val:\n",
    "                print(f\"│(clipping {param_name} real {ext} {v:.2} to size {clip_val})\")\n",
    "        for ext, v in [(\"min\", grad.imag.min()),(\"max\", grad.imag.max())]:\n",
    "            if verbose and abs(v) > clip_val:\n",
    "                print(f\"│(clipping {param_name} imag {ext} {1.j*v:.2} to size {clip_val})\")\n",
    "        clipped_grad = torch.complex(grad.real.clamp(-clip_val, clip_val),\n",
    "                                     grad.imag.clamp(-clip_val, clip_val))\n",
    "    else:\n",
    "        for ext, v in [(\"min\", grad.min()),(\"max\", grad.max())]:\n",
    "            if verbose and abs(v) > clip_val:\n",
    "                print(f\"│(clipping {param_name} {ext} {v:.2} to size {clip_val})\")\n",
    "        clipped_grad = torch.clamp(grad, -clip_val, clip_val)\n",
    "    return clipped_grad\n",
    "\n",
    "print(f\"Models: D={D}, d={d}\")\n",
    "for model in (*models, *models_hom):\n",
    "    print(f\"\\t{model.core.shape} model type: {model.name}\")\n",
    "    for param_index, p in enumerate(model.parameters()):\n",
    "        pnames = list(model.state_dict().keys())\n",
    "        p.register_hook(lambda grad: clip_grad(grad, 1000, pnames[param_index], verbose=True))\n",
    "        if torch.isnan(p).any():\n",
    "            print(f\"{pnames[param_index]} contains a NaN value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossvals_hom = train_models(models_hom, batchsize=10, max_epochs=1, optimizer=torch.optim.Adadelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossvals = train_models(models, batchsize=10, max_epochs=100)"
   ]
  },
  {
   "source": [
    "## Useful things?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch.autograd.set_detect_anomaly(True)\n",
    "# def train(self, dataset, batchsize, max_epochs, plot=True, **optim_kwargs):\n",
    "#     trainloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "#     optimizer = torch.optim.Adadelta(self.parameters(), **optim_kwargs)\n",
    "#     early_stopping_threshold = 1e-6 # min difference in epoch loss \n",
    "#     loss_values = [] # store by-epoch avg loss values\n",
    "#     print(f'╭───────────────────────────\\n│Training {self.name},')\n",
    "#     print(f'│         batchsize:{batchsize}, {optimizer.__module__}, {optim_kwargs}.')\n",
    "#     av_batch_loss_running = -1e4\n",
    "#     with tqdm(range(max_epochs), unit=\"epoch\", leave=True) as tepochs:\n",
    "#         for epoch in tepochs:\n",
    "#             batch_loss_list = []\n",
    "#             with tqdm(trainloader, unit=\"batch\", leave=False, desc=f\"epoch {epoch}\") as tepoch:\n",
    "#                 for batch in tepoch:\n",
    "#                     for pindex, p in enumerate(self.parameters()):\n",
    "#                         if torch.isnan(p).any():\n",
    "#                             pnames = list(self.state_dict().keys())\n",
    "#                             print(\"│ loss values:\", *(f\"{x:.3f}\" for x in loss_values))\n",
    "#                             print(f\"└────Stopped before epoch {epoch}. NaN in weights {pnames[pindex]}!\")\n",
    "#                             if plot:\n",
    "#                                 plt.plot(loss_values)\n",
    "#                                 plt.show()\n",
    "#                             return loss_values\n",
    "#                     self.zero_grad()\n",
    "#                     neglogprob = 0\n",
    "#                     for x in batch:\n",
    "#                         out = self(x)\n",
    "#                         neglogprob -= out\n",
    "#                     loss = neglogprob / len(batch)\n",
    "#                     loss.backward()\n",
    "#                     # for pindex, p in enumerate(self.parameters()):\n",
    "#                     #     if torch.isnan(p.grad).any():\n",
    "#                     #         pnames = list(self.state_dict().keys())\n",
    "#                     #         print(\"│ loss values:\", *(f\"{x:.3f}\" for x in loss_values))\n",
    "#                     #         print(f\"└────Stopped. NaN value in gradient for {pnames[pindex]}!\")\n",
    "#                     #         if plot:\n",
    "#                     #             plt.plot(loss_values)\n",
    "#                     #             plt.show()\n",
    "#                     #         return loss_values\n",
    "#                     optimizer.step()\n",
    "#                     tepoch.set_postfix(loss=loss.item())\n",
    "#                     batch_loss_list.append(loss.item())\n",
    "#                 av_batch_loss = torch.Tensor(batch_loss_list).mean().item()\n",
    "#                 loss_values.append(av_batch_loss)\n",
    "#                 tepochs.set_postfix(av_batch_loss=av_batch_loss)\n",
    "#                 if abs(av_batch_loss_running - av_batch_loss) < early_stopping_threshold:\n",
    "#                     print(f\"├────Early stopping after epoch {epoch}/{max_epochs}.\")\n",
    "#                     break\n",
    "#                 av_batch_loss_running = av_batch_loss\n",
    "#     print(\"│ loss values:\", *(f\"{x:.3f}\" for x in loss_values))\n",
    "#     if plot:\n",
    "#         plt.plot(loss_values)\n",
    "#         plt.show()\n",
    "#     print('│ Finished training.\\n╰───────────────────────────\\n')\n",
    "#     return loss_values\n",
    "\n",
    "# def train_models(models, batchsize, max_epochs, **optim_kwargs):\n",
    "#     print(f\"dataset: {dataset}\")\n",
    "#     models_loss_values={}\n",
    "#     for model in models:\n",
    "#         loss_values = train(model, X, batchsize=batchsize, plot=False, max_epochs = max_epochs, **optim_kwargs)\n",
    "#         plt.plot(loss_values, label=model.name)\n",
    "#         plt.ylabel('avg loss (NLL)')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.title(f\"dataset: {dataset} (d={d}), bond dim={D}\\n batchsize:{batchsize}, {optim_kwargs}\")\n",
    "#         models_loss_values[f\"{model.name}\"]=loss_values\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "#     plt.show()\n",
    "#     return models_loss_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}