{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensornetworks_pytorch.TNModels import PosMPS, Born\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import data\n",
    "import pickle\n",
    "for dataset in [#'biofam',\n",
    "    'flare','lymphography','spect','tumor','votes']:\n",
    "    with open('datasets/'+dataset, 'rb') as f:\n",
    "            a=pickle.load(f)\n",
    "    X=a[0].astype(int)\n",
    "    print(dataset)\n",
    "    print(\"\\tdata shape:\", X.shape)\n",
    "    print(f\"\\trange of X values: {X.min()} -- {X.max()}\")\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    with open('datasets/'+dataset, 'rb') as f:\n",
    "            a=pickle.load(f)\n",
    "    X=a[0]\n",
    "    X=X.astype(int)\n",
    "\n",
    "    print(\"\\tdata shape:\", X.shape)\n",
    "    print(f\"\\trange of X values: {X.min()} -- {X.max()} ==> d={X.max()+1}\")\n",
    "    d = X.max()+1\n",
    "    return X, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize models\n",
    "dataset = 'lymphography'\n",
    "print(\"dataset:\", dataset)\n",
    "X,d = load_dataset(dataset)\n",
    "\n",
    "D = 2\n",
    "mps      = PosMPS(X, d, D, homogeneous=False)\n",
    "mps_hom  = PosMPS(X, d, D, homogeneous=True)\n",
    "\n",
    "# mps_s    = PosMPS(X, d, D, homogeneous=False, log_stability=True)\n",
    "# mps_s_hom= PosMPS(X, d, D, homogeneous=True, log_stability=True)\n",
    "\n",
    "rBorn      = Born(X, d, D, dtype=torch.float, homogeneous=False, log_stability=False) \n",
    "rBorn_hom  = Born(X, d, D, dtype=torch.float, homogeneous=True, log_stability=False) \n",
    "\n",
    "rBorn_s    = Born(X, d, D, dtype=torch.float, homogeneous=False, log_stability=True) \n",
    "rBorn_s_hom= Born(X, d, D, dtype=torch.float, homogeneous=True, log_stability=True) \n",
    "\n",
    "cBorn      = Born(X, d, D, dtype=torch.cfloat, homogeneous=False, log_stability=False)\n",
    "cBorn_hom  = Born(X, d, D, dtype=torch.cfloat, homogeneous=True, log_stability=False)\n",
    "\n",
    "cBorn_s    = Born(X, d, D, dtype=torch.cfloat, homogeneous=False, log_stability=True)\n",
    "cBorn_s_hom= Born(X, d, D, dtype=torch.cfloat, homogeneous=True, log_stability=True)\n",
    "\n",
    "models     = (\n",
    "    rBorn, cBorn, rBorn_s, cBorn_s, mps#, mps_c\n",
    ")\n",
    "models_hom = (\n",
    "    rBorn_hom, cBorn_hom, rBorn_s_hom, cBorn_s_hom, mps_hom#, mps_c_hom\n",
    ")\n",
    "\n",
    "def clip_grad(grad, clip_val, param_name, verbose=False):\n",
    "    if torch.isnan(grad).any():\n",
    "        print(f\"│ Hook: NaN value in gradient of {param_name}, {grad.size()}\")\n",
    "    if grad.dtype==torch.cfloat:\n",
    "        for ext, v in [(\"min\", grad.real.min()),(\"max\", grad.real.max())]:\n",
    "            if verbose and abs(v) > clip_val:\n",
    "                print(f\"clipping real {ext} {v:.2} to size {clip_val}\")\n",
    "        for ext, v in [(\"min\", grad.imag.min()),(\"max\", grad.imag.max())]:\n",
    "            if verbose and abs(v) > clip_val:\n",
    "                print(f\"clipping imag {ext} {1.j*v:.2} to size {clip_val}\")\n",
    "        clipped_grad = torch.complex(grad.real.clamp(-clip_val, clip_val),\n",
    "                                     grad.imag.clamp(-clip_val, clip_val))\n",
    "    else:\n",
    "        for ext, v in [(\"min\", grad.min()),(\"max\", grad.max())]:\n",
    "            if verbose and abs(v) > clip_val:\n",
    "                print(f\"clipping {ext} {v:.2} to size {clip_val}\")\n",
    "        clipped_grad = torch.clamp(grad, -clip_val, clip_val)\n",
    "    return clipped_grad\n",
    "\n",
    "print(\"Initializing models:\")\n",
    "for model in (*models, *models_hom):\n",
    "    print(f\"\\t{model.core.shape} model type: {model.name}\")\n",
    "    clip_val = 1000\n",
    "    for param_index, p in enumerate(model.parameters()):\n",
    "        pnames = list(model.state_dict().keys())\n",
    "        p.register_hook(lambda grad: clip_grad(grad, clip_val, pnames[param_index], verbose=True))\n",
    "        if torch.isnan(p).any():\n",
    "            print(f\"{pnames[param_index]} contains a NaN value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, dataset, batchsize, max_epochs, plot=True, **optim_kwargs):\n",
    "    trainloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    optimizer = torch.optim.SGD(self.parameters(), **optim_kwargs)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "    early_stopping_threshold = 0.0001 # min difference in epoch loss\n",
    "    loss_values = [] # store by-epoch avg loss values\n",
    "    print('╭───────────────────────────')\n",
    "    print(f\"│Training {self.name},\")\n",
    "    print(f\"│  batchsize:{batchsize} opt:{optim_kwargs}.\")\n",
    "    av_batch_loss_running = -1000\n",
    "    with tqdm(range(max_epochs), leave=True) as tepochs:\n",
    "        for epoch in tepochs:\n",
    "            batch_loss = []\n",
    "            with tqdm(trainloader, unit=\"batch\", leave=False, desc=f\"epoch {epoch}\") as tepoch:\n",
    "                for batch in tepoch:\n",
    "                    for p in self.parameters():\n",
    "                        if torch.isnan(p).any():\n",
    "                            print(\"│ loss values:\", *(f\"{x:.3f}\" for x in loss_values))\n",
    "                            print(\"└────Stopped. After updating, model weights contain a NaN value!\")\n",
    "                            if plot:\n",
    "                                plt.plot(loss_values)\n",
    "                                plt.show()\n",
    "                            return loss_values\n",
    "                    self.zero_grad()\n",
    "                    neglogprob = 0\n",
    "                    for i,x in enumerate(batch):\n",
    "                        out = self(x)\n",
    "                        neglogprob -= out\n",
    "                    loss = neglogprob / len(batch)\n",
    "#                     scheduler.step(loss)\n",
    "                    loss.backward()\n",
    "                    for p in self.parameters():\n",
    "                        if torch.isnan(p.grad).any():\n",
    "                            print(\"│ loss values:\", *(f\"{x:.3f}\" for x in loss_values))\n",
    "                            print(\"└────Stopped. Gradient contains a NaN value!\")\n",
    "                            if plot:\n",
    "                                plt.plot(loss_values)\n",
    "                                plt.show()\n",
    "                            return loss_values\n",
    "                    optimizer.step()\n",
    "                    tepoch.set_postfix(loss=loss.item())\n",
    "                    with torch.no_grad():\n",
    "                        batch_loss.append(loss.item())\n",
    "            av_batch_loss = torch.Tensor(batch_loss).mean().item()\n",
    "            #print(f\"ep{epoch} av_batch_loss\\t {av_batch_loss}\")\n",
    "            loss_values.append(av_batch_loss)\n",
    "            tepochs.set_postfix(av_batch_loss=av_batch_loss)\n",
    "            if abs(av_batch_loss_running - av_batch_loss) < early_stopping_threshold:\n",
    "                print(\"└────Early stopping.\")\n",
    "                break\n",
    "            av_batch_loss_running = av_batch_loss\n",
    "    print(\"│ loss values:\", *(f\"{x:.3f}\" for x in loss_values))\n",
    "    if plot:\n",
    "        plt.plot(loss_values)\n",
    "        plt.show()\n",
    "    print('╰────────Finished─training──\\n')\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelhom_loss_values={}\n",
    "# for model in [rBorn_hom]:\n",
    "#     loss_values = train(model, X, batchsize=20, plot=False, max_epochs=50, lr=0.1)\n",
    "#     plt.plot(loss_values, label=model.name)\n",
    "#     plt.ylabel('avg loss (NLL)')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.title(f\"dataset: {dataset} (d={d}), bond dim={D}\")\n",
    "#     modelhom_loss_values[\"model.name\"]=loss_values\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelhom_loss_values={}\n",
    "batchsize=35\n",
    "max_epochs=100\n",
    "optim_kwargs = dict(lr=0.1)\n",
    "for model in models_hom:\n",
    "    loss_values = train(model, X, batchsize=batchsize, plot=False, max_epochs=max_epochs, **optim_kwargs)\n",
    "    plt.plot(loss_values, label=model.name)\n",
    "    plt.ylabel('avg loss (NLL)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title(f\"dataset: {dataset} (d={d}), bond dim={D}\\n batchsize:{batchsize}, {optim_kwargs}\")\n",
    "    modelhom_loss_values[\"model.name\"]=loss_values\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelhom_loss_values={}\n",
    "batchsize=35\n",
    "max_epochs=100\n",
    "optim_kwargs = dict(lr=0.05)\n",
    "for model in models:\n",
    "    loss_values = train(model, X, batchsize=batchsize, plot=False, max_epochs = max_epochs, **optim_kwargs)\n",
    "    plt.plot(loss_values, label=model.name)\n",
    "    plt.ylabel('avg loss (NLL)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title(f\"dataset: {dataset} (d={d}), bond dim={D}\\n batchsize:{batchsize}, {optim_kwargs}\")\n",
    "    modelhom_loss_values[\"model.name\"]=loss_values\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Useful things?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTrain(nn.Module):\n",
    "    \"\"\"Abstract class for Tensor Train models.  Use instantiating class.\n",
    "\n",
    "    Parameters:\n",
    "        D (int): bond dimension\n",
    "        d (int): physical dimension (number of categories in data)\n",
    "        dtype ([tensor.dtype]): \n",
    "            tensor.float for real, or tensor.cfloat for complex\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, d, D, dtype, homogeneous=True, verbose=False):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.d = d\n",
    "        self.verbose = verbose\n",
    "        self.homogeneous = homogeneous\n",
    "        self.n_datapoints = dataset.shape[0]\n",
    "        self.seqlen = dataset.shape[1]\n",
    "\n",
    "        w_init = self.randomsign_ones # alternatively, use torch.ones\n",
    "\n",
    "        # the following are set to nn.Parameters thus are backpropped over\n",
    "        k_core = (d*D*D)**-0.5 \n",
    "        k_vectors = (d)**-0.5\n",
    "        # TODO k should be (d*D*D)**-0.5, \n",
    "        # we should use randn instead of rand, \n",
    "        # but this seems to make more NaNs for the homogeneous models.\n",
    "        if homogeneous: # initialize single core to be repeated\n",
    "            core = k_core * w_init(\n",
    "                (d, D, D), dtype=dtype)\n",
    "            self.core = nn.Parameter(core)\n",
    "        else: # initialize seqlen different non-homogeneous cores\n",
    "            core = k_core * w_init(\n",
    "                (self.seqlen, d, D, D), dtype=dtype)\n",
    "            self.core = nn.Parameter(core)\n",
    "        self.left_boundary = nn.Parameter(\n",
    "            k_vectors * w_init((D), dtype=dtype))\n",
    "        self.right_boundary = nn.Parameter(\n",
    "            k_vectors * w_init((D), dtype=dtype))\n",
    "    \n",
    "    @staticmethod\n",
    "    def randomsign_ones(shape, dtype=torch.float):\n",
    "        \"\"\"Makes a vector of ones with random sign, \n",
    "        or if dtype is torch.cfloat, randomized real or imaginary units\"\"\"\n",
    "        x = torch.zeros(shape)\n",
    "        if dtype==torch.cfloat:\n",
    "            random4=torch.randint_like(x,4)\n",
    "            r = x + 1*(random4==0) - 1*(random4==1) \n",
    "            i = x + 1*(random4==2) - 1*(random4==3)\n",
    "            out = torch.complex(r,i)\n",
    "        else:\n",
    "            random2=torch.randint_like(x,2)\n",
    "            out = x + 1*(random2==0) - 1*(random2==1) \n",
    "        return torch.tensor(out, dtype=dtype)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}